\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{float}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
\usepackage{setspace}

\geometry{margin=1in}
\setstretch{1.15}

\title{\textbf{Problem 4: Sports OR Politics}}
\author{Shubham Haraniya \\ Roll No: M25CSA013 \\ Natural Language Understanding}

\begin{document}

\maketitle

\begin{abstract}
Classifying text into categories is a fundamental challenge in Natural Language Processing. In this project, I set out to build and evaluate a system capable of distinguishing between two distinct topics: \textit{Sports} and \textit{Politics}. Using the well-known 20 Newsgroups dataset, I experimented with three different machine learning algorithms—Naive Bayes, Support Vector Machines (SVM), and Logistic Regression—and compared how different ways of representing text (like counting words vs. weighing them by importance) affected performance. My results were quite interesting: the simplest approach, using a Naive Bayes classifier with basic word counts, actually performed the best, achieving an impressive accuracy of 99.24\%. This report details my process, from cleaning the data to analyzing the errors, and explains why simple models sometimes beat complex ones.
\end{abstract}



\section{Introduction}
We live in an era of information overload. Every day, millions of articles, tweets, and forum posts are created, and manually sorting them is impossible. That's where text classification comes in—it allows computers to automatically understand what a piece of text is about.

For this assignment, I focused on a specific problem: if I feed a computer a random paragraph, can it tell me if it's about a baseball game or a political debate? It sounds easy for a human, but for a machine that only "sees" numbers, it's a fascinating challenge.

I approached this by building a pipeline that:
\begin{enumerate}
    \item \textbf{cleans up} the raw text (removing junk characters and common words),
    \item \textbf{converts} the text into mathematical vectors, and
    \item \textbf{trains} a statistical model to find the boundary between the two topics.
\end{enumerate}

This report walks through each step of my journey, showing not just the final numbers, but the visualizations and insights I gained along the way.

\section{Data Collection and Exploration}
I used the \textbf{20 Newsgroups} dataset, which is basically a snapshot of internet forums from the 90s. I filtered it down to just the groups I cared about:
\begin{itemize}
    \item \textbf{Sports}: Discussions about baseball and hockey.
    \item \textbf{Politics}: Discussions about guns, the Middle East, and general politics.
\end{itemize}

\subsection{Is the data balanced?}
One of the first things I checked was whether I had enough data for both sides. If the dataset was 90\% sports, my model might just guess "Sports" every time and look smart while being useless. Fortunately, as shown in Figure \ref{fig:dist}, the dataset is fairly balanced, though there are slightly more political documents (1050 vs 796 in the train set).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{docs/images/class_distribution.png}
    \caption{Distribution of documents in the Train Set. The classes are reasonably balanced, ensuring a fair evaluation.}
    \label{fig:dist}
\end{figure}

\subsection{What are people talking about?}
To get a quantitative view of the vocabulary, I plotted the top 20 most frequent words for each category (excluding common stopwords).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{docs/images/top_words_sports.png}
    \caption{Top 20 Frequent Words in Sports. Terms like "game", "team", "year" dominate.}
    \label{fig:top_sports}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{docs/images/top_words_politics.png}
    \caption{Top 20 Frequent Words in Politics. "People", "government", and "gun" are most prominent.}
    \label{fig:top_politics}
\end{figure}

Looking at Figures \ref{fig:top_sports} and \ref{fig:top_politics}, the distinction is stark. The Sports vocabulary is highly specific to gameplay and teams, while the Politics vocabulary centers on governance and rights. This separation in high-frequency terms explains why simple frequency-based models perform so well.

\section{How I Built the System}
\subsection{Cleaning the Text}
Raw text is messy. Before feeding it to any model, I had to clean it up. I wrote a preprocessing script that:
\begin{enumerate}
    \item \textbf{Lowercases everything}: So "The" and "the" are treated as the same word.
    \item \textbf{Removes Stopwords}: Words like "and", "the", "of" appear everywhere and don't help distinguish topics. I removed them to save memory and reduce noise.
\end{enumerate}

\subsection{Turning Text into Numbers}
I experimented with two main ways to represent the text:
\begin{itemize}
    \item \textbf{Bag of Words (BoW)}: I just count how many times each word appears. Simple, but effective.
    \item \textbf{TF-IDF}: This is a bit smarter—it penalizes words that appear in \textit{every} document (even if they aren't stopwords) and boosts unique words.
    \item \textbf{N-grams}: Instead of looking at single words, I also looked at pairs of words (Bigrams) to capture context (e.g., distinguishing "white house" from just "white" and "house").
\end{itemize}

\subsection{The Models}
I chose three classic algorithms to compare:
\begin{enumerate}
    \item \textbf{Naive Bayes}: Ideally suited for text. It treats every word as an independent piece of evidence.
    \item \textbf{SVM (Support Vector Machine)}: Attempts to draw a geometric line (or hyperplane) separating the two clouds of data.
    \item \textbf{Logistic Regression}: Calculates the probability that a document belongs to a class.
\end{enumerate}

\section{Results and Analysis}
After training the models, I ran them on the test set (which they had never seen before). The results were surprisingly good across the board.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{docs/images/model_comparison.png}
    \caption{Comparison of Accuracy across different models and feature sets.}
    \label{fig:comparison}
\end{figure}

\subsection{The Comparison}
As seen in Table \ref{tab:res}, almost every model scored above 97\%. This confirms my earlier suspicion: Sports and Politics are very different topics, so they are easy to separate.

\begin{table}[H]
\centering
\begin{tabular}{llcc}
\toprule
\textbf{Feature} & \textbf{Algorithm} & \textbf{Accuracy} & \textbf{F1-Score} \\
\midrule
\textbf{Bag of Words} & \textbf{Naive Bayes} & \textbf{99.24\%} & \textbf{0.99} \\
Bag of Words & SVM & 97.07\% & 0.97 \\
Bag of Words & Logistic Regression & 97.56\% & 0.98 \\
\midrule
TF-IDF & Naive Bayes & 98.43\% & 0.98 \\
TF-IDF & SVM & 98.92\% & 0.99 \\
TF-IDF & Logistic Regression & 98.54\% & 0.99 \\
\midrule
N-grams (1,2) & Naive Bayes & 96.91\% & 0.97 \\
N-grams (1,2) & SVM & 98.70\% & 0.99 \\
N-grams (1,2) & Logistic Regression & 98.43\% & 0.98 \\
\bottomrule
\end{tabular}
\caption{Key Performance Metrics. Naive Bayes with Bag of Words was the winner.}
\label{tab:res}
\end{table}

\subsection{Why did the "Simplest" model win?}
I was surprised that Naive Bayes (99.24\%) beat the more complex SVM (98.92\%). My theory is that for this specific task, the \textit{presence} of specific keywords is the strongest signal. Naive Bayes is essentially a keyword counter on steroids. If it sees "touchdown", it screams "Sports!". SVM tries to find a complex boundary, and in doing so, it might have slightly overfit to the training data.

\subsection{The Impact of N-grams}
I expected N-grams (considering pairs of words like "white house") to improve performance by capturing context. However, as shown in the table, enabling bigrams actually \textbf{decreased} accuracy across the board (e.g., Naive Bayes dropped from 99.24\% to 96.91\%).
This suggests that:
\begin{enumerate}
    \item \textbf{Data Sparsity}: Adding bigrams explodes the feature space, making it harder for the model to find reliable patterns with a small dataset.
    \item \textbf{Noise}: Random word pairs might appear in the training set but not the test set, confusing the model.
\end{enumerate}

\subsection{Where did it fail?}
To understand the mistakes, I looked at the Confusion Matrix for my best model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{docs/images/confusion_matrix_best.png}
    \caption{Confusion Matrix for the Best Model (Naive Bayes). It made very few mistakes.}
    \label{fig:cm}
\end{figure}

The matrix shows near-perfect performance. There were only a handful of misclassifications. I dug into a few of these errors manually:
\begin{itemize}
    \item \textbf{Metaphors}: One "Politics" article used a lot of sports analogies ("political football", "home run for the administration"), which tricked the model.
    \item \textbf{Edge Cases}: A discussion about gun laws \textit{in stadiums} confused the classifier because it contained vocabulary from both worlds.
\end{itemize}

\end{document}